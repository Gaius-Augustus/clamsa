{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_tcmc.tcmc.tcmc import TCMCProbability\n",
    "import utilities.onehot_tuple_encoder as ote\n",
    "from utilities import database_reader\n",
    "from utilities import msa_converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the Location of the Dataset and the wanted Splits and import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <ShuffleDataset shapes: ((), (), (), (None, None, None)), types: (tf.int64, tf.int64, tf.int64, tf.float64)>, 'val': {0: <ParallelMapDataset shapes: ((), (), (), (None, None, None)), types: (tf.int64, tf.int64, tf.int64, tf.float64)>, 1: <ParallelMapDataset shapes: ((), (), (), (None, None, None)), types: (tf.int64, tf.int64, tf.int64, tf.float64)>}, 'test': {0: <ParallelMapDataset shapes: ((), (), (), (None, None, None)), types: (tf.int64, tf.int64, tf.int64, tf.float64)>, 1: <ParallelMapDataset shapes: ((), (), (), (None, None, None)), types: (tf.int64, tf.int64, tf.int64, tf.float64)>}}\n"
     ]
    }
   ],
   "source": [
    "folder = '../msa/'\n",
    "basename = 'augustus_flies'\n",
    "forest = ['../clades/flies.nwk']\n",
    "num_leaves = database_reader.num_leaves(forest)\n",
    "\n",
    "used_codons = True\n",
    "entry_length = 3 if used_codons else 1\n",
    "alphabet_size = 4 ** entry_length\n",
    "\n",
    "train_split = database_reader.DatasetSplitSpecification(\n",
    "    name = 'train', \n",
    "    wanted_models = [0, 1],\n",
    "    interweave_models = [.62, .38],\n",
    "    repeat_models = [False, True]\n",
    ")\n",
    "\n",
    "val_split = database_reader.DatasetSplitSpecification(\n",
    "    name = 'val',\n",
    "    wanted_models = [0, 1]\n",
    ")\n",
    "\n",
    "test_split = database_reader.DatasetSplitSpecification(\n",
    "    name = 'test',\n",
    "    wanted_models = [0, 1]\n",
    ")\n",
    "\n",
    "\n",
    "wanted_splits = [train_split, val_split, test_split]\n",
    "\n",
    "# gather the dict of splits\n",
    "datasets = database_reader.get_datasets(folder, basename, wanted_splits, num_leaves = num_leaves, alphabet_size = alphabet_size, seed = None, buffer_size = 1000)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1\n",
      "clade_id: 0\n",
      "sequence_length: 129\n",
      "sequence_onehot.shape: (129, 12, 64)\n",
      "first (up to) 8 alignment columns of decoded reshaped sequence: \n",
      "[['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['atg' 'tgt' 'tcc' 'aaa' 'cta' 'act' 'ctt' 'ttc']\n",
      " ['atg' 'tgt' 'tcc' 'aaa' 'ctc' 'acc' 'ctt' 'ttc']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['atg' 'tgt' 'tcc' 'aaa' 'ctc' 'gtt' 'ctt' 'ttc']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']\n",
      " ['---' '---' '---' '---' '---' '---' '---' '---']]\n"
     ]
    }
   ],
   "source": [
    "# for debug purposes print one entry\n",
    "for t in datasets[train_split.name].take(1):\n",
    "    model, clade_id, sequence_length, sequence_onehot = t\n",
    "\n",
    "    print(f'model: {model}')\n",
    "    print(f'clade_id: {clade_id}')\n",
    "    print(f'sequence_length: {sequence_length}')\n",
    "    print(f'sequence_onehot.shape: {sequence_onehot.shape}')\n",
    "    \n",
    "    # to debug the sequence first transform it to its\n",
    "    # original shape\n",
    "    S = tf.transpose(sequence_onehot, perm = [1, 0, 2])\n",
    "    \n",
    "    # decode the sequence and print some columns\n",
    "    dec = ote.OnehotTupleEncoder.decode_tfrecord_entry(S.numpy(), tuple_length = entry_length)\n",
    "    print(f'first (up to) 8 alignment columns of decoded reshaped sequence: \\n{dec[:,:8]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
